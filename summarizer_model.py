# -*- coding: utf-8 -*-
"""draft+api

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1geebdx60V4zEa6FNfxlkc_glWBy0Jspe

# Resources

resouces : https://docs.cohere.com/v2/changelog/command-r7b-arabic

translator Model resource : https://huggingface.co/Helsinki-NLP/opus-mt-ar-en/tree/main

# convert notebook to ipynb
"""

!jupyter nbconvert --to script your_notebook_name.ipynb

"""# pips"""

!pip install -q fastapi nest-asyncio pyngrok uvicorn
!pip install -q bitsandbytes
!pip install -q accelerate==0.26.0 --progress-bar off
!pip install fastapi nest-asyncio pyngrok uvicorn

# !pip install -U -q bitsandbytes transformers accelerate

"""# Import Liberaries"""

from pydantic import BaseModel, Field
from typing import List, Optional, Literal
import json
from collections import OrderedDict
from transformers import AutoTokenizer, AutoModelForCausalLM , BitsAndBytesConfig
import torch
import torch._dynamo
import os
from google.colab import drive
from huggingface_hub import login
import nest_asyncio
from pyngrok import ngrok
import uvicorn
from google.colab import userdata
nest_asyncio.apply()
hf_token  = "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"
base_model_id = "CohereForAI/c4ai-command-r7b-arabic-02-2025"
torch_dtype = torch.float16
login(token=hf_token)
torch._dynamo.config.suppress_errors = True
torch._dynamo.config.disable = True

"""# Read Json file"""

# drive.mount('/content/gdrive')

# # Define the file path
# file_path = "/content/gdrive/MyDrive/medical_conv.json"

# # Check if file exists before reading
# if os.path.exists(file_path):
#     with open(file_path, "r", encoding="utf-8") as json_file:
#         try:
#             data = json.load(json_file)

#             # Create a list to store formatted conversations
#             conversation_list = [f"{item['conversation']}".replace("\n" , " ") for i, item in enumerate(data, start=1)]

#             # Print the list
#             print(conversation_list)

#         except json.JSONDecodeError as e:
#             print(f"Error decoding JSON: {e}")
# else:
#     print("File not found!")

lognest_cnversation = """
Doctor: ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±! Ù…Ø§ Ø§Ù„Ø°ÙŠ Ø£ØªÙ‰ Ø¨Ùƒ Ø§Ù„ÙŠÙˆÙ…ØŸ

Patient: ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±ØŒ Ø¯ÙƒØªÙˆØ±. Ù„Ø§Ø­Ø¸Øª ÙˆØ¬ÙˆØ¯ Ø¨Ù‚Ø¹Ø© Ø®Ø´Ù†Ø© ÙˆÙ…ØªÙ‚Ø´Ø±Ø© Ø¹Ù„Ù‰ Ø¬Ø¨Ù‡ØªÙŠ Ù…Ù†Ø° Ø¨Ø¶Ø¹Ø© Ø£Ø´Ù‡Ø±. Ø¨Ø¯Ø£Øª ØµØºÙŠØ±Ø©ØŒ Ù„ÙƒÙ†Ù†ÙŠ Ø£Ø¹ØªÙ‚Ø¯ Ø£Ù†Ù‡Ø§ ØªÙƒØ¨Ø±.

Doctor: ÙÙ‡Ù…Øª. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ ÙˆØµÙ ÙƒÙŠÙ ØªØ´Ø¹Ø± Ø¨Ù‡Ø§ØŸ Ù‡Ù„ Ù‡ÙŠ Ù…Ø«ÙŠØ±Ø© Ù„Ù„Ø­ÙƒØ©ØŒ Ù…Ø¤Ù„Ù…Ø©ØŒ Ø£Ù… Ø­Ø³Ø§Ø³Ø© Ø¹Ù†Ø¯ Ù„Ù…Ø³Ù‡Ø§ØŸ

Patient: Ø¥Ù†Ù‡Ø§ Ø®Ø´Ù†Ø© ÙˆØ¬Ø§ÙØ© ÙÙŠ Ø§Ù„ØºØ§Ù„Ø¨. Ø£Ø­ÙŠØ§Ù†Ù‹Ø§ Ø£Ø´Ø¹Ø± Ø¨Ø­ÙƒØ© Ø®ÙÙŠÙØ©ØŒ ÙˆØ¥Ø°Ø§ Ø­ÙƒÙƒØªÙ‡Ø§ ÙƒØ«ÙŠØ±Ù‹Ø§ØŒ ØªÙ†Ø²Ù Ù‚Ù„ÙŠÙ„Ù‹Ø§.

Doctor: Ù…Ù†Ø° Ù…ØªÙ‰ ÙˆØ£Ù†Øª ØªÙ„Ø§Ø­Ø¸ Ù‡Ø°Ø§ Ø§Ù„ØªØºÙŠØ±ØŸ Ù‡Ù„ Ù„Ø§Ø­Ø¸Øª Ø£ÙŠ ØªØºÙŠÙŠØ±Ø§Øª ÙÙŠ Ø§Ù„Ø­Ø¬Ù… Ø£Ùˆ Ø§Ù„Ù„ÙˆÙ† Ø£Ùˆ Ø§Ù„Ù…Ù„Ù…Ø³ Ø¨Ù…Ø±ÙˆØ± Ø§Ù„ÙˆÙ‚ØªØŸ

Patient: Ù„Ø§Ø­Ø¸ØªÙ‡Ø§ Ù„Ø£ÙˆÙ„ Ù…Ø±Ø© Ù…Ù†Ø° Ø­ÙˆØ§Ù„ÙŠ Ø³ØªØ© Ø£Ø´Ù‡Ø±. ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©ØŒ ÙƒØ§Ù†Øª Ø¨Ø§Ù„ÙƒØ§Ø¯ ØªÙØ±Ù‰ØŒ Ù„ÙƒÙ†Ù‡Ø§ Ø§Ù„Ø¢Ù† Ø£ØµØ¨Ø­Øª Ø£ÙƒØ«Ø± Ø®Ø´ÙˆÙ†Ø© ÙˆÙŠØ¨Ø¯Ùˆ Ø£Ù†Ù‡Ø§ Ø£ÙƒØ¨Ø± Ù‚Ù„ÙŠÙ„Ø§Ù‹.

Doctor: Ù‡Ù„ Ø³Ø¨Ù‚ Ù„Ùƒ Ø£Ù† Ø¹Ø§Ù†ÙŠØª Ù…Ù† Ø¨Ù‚Ø¹ Ù…Ø´Ø§Ø¨Ù‡Ø© Ø¹Ù„Ù‰ Ø¨Ø´Ø±ØªÙƒ Ù…Ù† Ù‚Ø¨Ù„ØŸ

Patient: Ù„Ø§ØŒ Ù‡Ø°Ù‡ Ù‡ÙŠ Ø§Ù„Ù…Ø±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ø§Ù„ØªÙŠ ÙŠØ­Ø¯Ø« Ù„ÙŠ ÙÙŠÙ‡Ø§ Ø´ÙŠØ¡ ÙƒÙ‡Ø°Ø§.

Doctor: Ù‡Ù„ ØªÙ‚Ø¶ÙŠ ÙˆÙ‚ØªÙ‹Ø§ Ø·ÙˆÙŠÙ„Ø§Ù‹ ÙÙŠ Ø§Ù„Ù‡ÙˆØ§Ø¡ Ø§Ù„Ø·Ù„Ù‚ØŸ Ù‡Ù„ ØªØ¹Ø±Ø¶Øª Ù„Ø£Ø´Ø¹Ø© Ø§Ù„Ø´Ù…Ø³ Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ± Ø¹Ù„Ù‰ Ù…Ø± Ø§Ù„Ø³Ù†ÙŠÙ†ØŸ

Patient: Ù†Ø¹Ù…ØŒ Ø£Ø¹Ù…Ù„ ÙÙŠ Ø§Ù„Ø¨Ù†Ø§Ø¡ØŒ Ù„Ø°Ø§ Ø£ØªØ¹Ø±Ø¶ Ù„Ù„Ø´Ù…Ø³ ÙŠÙˆÙ…ÙŠÙ‹Ø§ Ù„Ø³Ø§Ø¹Ø§Øª Ø·ÙˆÙŠÙ„Ø©.

Doctor: Ù‡Ù„ ØªØ³ØªØ®Ø¯Ù… ÙˆØ§Ù‚ÙŠ Ø§Ù„Ø´Ù…Ø³ Ø¨Ø§Ù†ØªØ¸Ø§Ù… Ø£Ùˆ ØªØ±ØªØ¯ÙŠ Ù…Ù„Ø§Ø¨Ø³ ÙˆØ§Ù‚ÙŠØ© Ù…Ø«Ù„ Ø§Ù„Ù‚Ø¨Ø¹Ø§ØªØŸ

Patient: Ø¨ØµØ±Ø§Ø­Ø©ØŒ Ù„Ø§. Ø£Ø±ØªØ¯ÙŠ Ù‚Ø¨Ø¹Ø© Ø£Ø­ÙŠØ§Ù†Ù‹Ø§ØŒ Ù„ÙƒÙ† Ù„Ø§ Ø£Ø³ØªØ®Ø¯Ù… ÙˆØ§Ù‚ÙŠ Ø§Ù„Ø´Ù…Ø³. Ù„Ù… Ø£ÙƒÙ† Ø£Ø¹ØªÙ‚Ø¯ Ø£Ù†Ù‡ Ù…Ù‡Ù….

Doctor: Ø§Ù„ØªØ¹Ø±Ø¶ Ù„Ø£Ø´Ø¹Ø© Ø§Ù„Ø´Ù…Ø³ Ø¹Ø§Ù…Ù„ Ø±Ø¦ÙŠØ³ÙŠ ÙÙŠ ØªÙ„Ù Ø§Ù„Ø¬Ù„Ø¯ØŒ ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ§Ù‚ÙŠ Ø§Ù„Ø´Ù…Ø³ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙ‚Ù„Ù„ Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ± Ù…Ù† Ø®Ø·Ø± Ø§Ù„Ø¥ØµØ§Ø¨Ø© Ø¨Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø¬Ù„Ø¯ØŒ Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ Ø§Ù„Ø¢ÙØ§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù„Ù„ØªØ³Ø±Ø·Ù† ÙˆØ³Ø±Ø·Ø§Ù† Ø§Ù„Ø¬Ù„Ø¯.

Patient: Ù„Ù… Ø£ÙƒÙ† Ø£Ø¹Ù„Ù… Ø£Ù†Ù‡ Ø£Ù…Ø± Ø®Ø·ÙŠØ± Ù„Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø±Ø¬Ø©. Ù‡Ù„ ØªØ¹ØªÙ‚Ø¯ Ø£Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø¨Ù‚Ø¹Ø© Ù‚Ø¯ ØªÙƒÙˆÙ† Ø®Ø·ÙŠØ±Ø©ØŸ

Doctor: Ù„Ø§ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„Ø¬Ø²Ù… Ø¨Ø°Ù„Ùƒ Ø¯ÙˆÙ† ÙØ­Øµ Ø¯Ù‚ÙŠÙ‚ØŒ ÙˆÙ„ÙƒÙ† Ù…Ù† Ø§Ù„Ø¬ÙŠØ¯ Ø£Ù†Ùƒ Ø¬Ø¦Øª Ù…Ø¨ÙƒØ±Ù‹Ø§. Ø¨Ø¹Ø¶ Ø¢ÙØ§Øª Ø§Ù„Ø¬Ù„Ø¯ Ù‚Ø¯ ØªÙƒÙˆÙ† ØºÙŠØ± Ø¶Ø§Ø±Ø©ØŒ ÙˆÙ„ÙƒÙ† Ø§Ù„Ø¨Ø¹Ø¶ Ø§Ù„Ø¢Ø®Ø± Ù‚Ø¯ ÙŠØªØ·Ù„Ø¨ Ø¹Ù„Ø§Ø¬Ù‹Ø§ Ù„Ù…Ù†Ø¹ Ø­Ø¯ÙˆØ« Ù…Ø¶Ø§Ø¹ÙØ§Øª.

Patient: Ù‡Ø°Ø§ Ù…Ù‚Ù„Ù‚ Ø¨Ø¹Ø¶ Ø§Ù„Ø´ÙŠØ¡. Ù‡Ù„ Ù‡Ø°Ø§ ÙŠØ¹Ù†ÙŠ Ø£Ù†Ù‡ Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø³Ø±Ø·Ø§Ù†Ù‹Ø§ØŸ

Doctor: Ù„ÙŠØ³ Ø¨Ø§Ù„Ø¶Ø±ÙˆØ±Ø©ØŒ ÙˆÙ„ÙƒÙ† Ø¨Ø¹Ø¶ Ø£Ù†ÙˆØ§Ø¹ Ø³Ø±Ø·Ø§Ù† Ø§Ù„Ø¬Ù„Ø¯ ØªØ¨Ø¯Ø£ ÙƒØ¨Ù‚Ø¹ Ø®Ø´Ù†Ø© ÙˆÙ…ØªÙ‚Ø´Ø±Ø© Ù…Ø«Ù„ Ù…Ø§ ØªØµÙÙ‡. Ø³Ø£Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ÙØ­ØµÙ‡Ø§ Ø¹Ù† ÙƒØ«Ø¨ ÙˆØ±Ø¨Ù…Ø§ Ø£Ø®Ø° Ø®Ø²Ø¹Ø© Ù„ØªØ£ÙƒÙŠØ¯ Ø§Ù„ØªØ´Ø®ÙŠØµ.

Patient: Ø­Ø³Ù†Ù‹Ø§ØŒ Ù‡Ø°Ø§ Ù…Ù†Ø·Ù‚ÙŠ. Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ø£Ø¹Ø±Ø§Ø¶ Ø£Ø®Ø±Ù‰ ÙŠØ¬Ø¨ Ø£Ù† Ø£Ø±Ø§Ù‚Ø¨Ù‡Ø§ØŸ

Doctor: Ù†Ø¹Ù…ØŒ Ø¥Ø°Ø§ Ø¨Ø¯Ø£Øª Ø§Ù„Ø¨Ù‚Ø¹Ø© ÙÙŠ Ø§Ù„ØªØºÙŠØ± Ø¨Ø³Ø±Ø¹Ø© ÙÙŠ Ø§Ù„Ø­Ø¬Ù… Ø£Ùˆ Ø§Ù„Ù„ÙˆÙ† Ø£Ùˆ Ø§Ù„Ø´ÙƒÙ„ØŒ Ø£Ùˆ Ø¥Ø°Ø§ Ø£ØµØ¨Ø­Øª Ù…Ø¤Ù„Ù…Ø© Ø£Ùˆ Ø¨Ø¯Ø£Øª ÙÙŠ Ø§Ù„Ø¥ÙØ±Ø§Ø² Ø£Ùˆ ØªØ·ÙˆØ±Øª Ø¥Ù„Ù‰ Ù‚Ø±Ø­Ø©ØŒ ÙÙ‚Ø¯ ØªÙƒÙˆÙ† Ù‡Ø°Ù‡ Ø¹Ù„Ø§Ù…Ø§Øª ØªØ­Ø°ÙŠØ±ÙŠØ©.

Patient: ÙÙ‡Ù…Øª. Ø­Ø³Ù†Ù‹Ø§ØŒ Ù„Ù… Ø£Ù„Ø§Ø­Ø¸ Ø£ÙŠ Ø¥ÙØ±Ø§Ø²Ø§Øª Ø£Ùˆ ØªØºÙŠØ±Ø§Øª ÙƒØ¨ÙŠØ±Ø© ÙÙŠ Ø§Ù„Ù„ÙˆÙ†.

Doctor: Ù‡Ø°Ø§ Ø¬ÙŠØ¯. Ù‡Ù„ Ù„Ø§Ø­Ø¸Øª Ø¸Ù‡ÙˆØ± Ø£ÙŠ Ø´Ø§Ù…Ø§Øª Ø£Ùˆ Ø¨Ù‚Ø¹ Ø¬Ø¯ÙŠØ¯Ø© Ø¹Ù„Ù‰ Ø¬Ø³Ù…Ùƒ Ù…Ø¤Ø®Ø±Ù‹Ø§ØŸ

Patient: Ù„Ø§ØŒ Ù„ÙŠØ³ Ø¹Ù„Ù‰ Ø­Ø¯ Ø¹Ù„Ù…ÙŠ.

Doctor: Ù‡Ù„ Ù„Ø¯ÙŠÙƒ ØªØ§Ø±ÙŠØ® Ø³Ø§Ø¨Ù‚ Ù…Ø¹ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø¬Ù„Ø¯ Ù…Ø«Ù„ Ø§Ù„Ø·ÙØ­ Ø§Ù„Ø¬Ù„Ø¯ÙŠ Ø£Ùˆ Ø§Ù„Ø£ÙƒØ²ÙŠÙ…Ø§ØŸ

Patient: Ù„Ø§ØŒ Ø¨Ø´Ø±ØªÙŠ ÙƒØ§Ù†Øª Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ø·Ø¨ÙŠØ¹ÙŠØ©.

Doctor: Ù‡Ù„ ÙŠÙˆØ¬Ø¯ ÙÙŠ Ø¹Ø§Ø¦Ù„ØªÙƒ Ø£ÙŠ Ø´Ø®Øµ Ù„Ø¯ÙŠÙ‡ ØªØ§Ø±ÙŠØ® Ù…Ù† Ø³Ø±Ø·Ø§Ù† Ø§Ù„Ø¬Ù„Ø¯ Ø£Ùˆ Ø£ÙŠ Ù…Ø´Ø§ÙƒÙ„ Ø¬Ù„Ø¯ÙŠØ© Ø£Ø®Ø±Ù‰ØŸ

Patient: ÙˆØ§Ù„Ø¯ÙŠ Ø£Ø²Ø§Ù„ Ø¨Ø¹Ø¶ Ø§Ù„Ø¨Ù‚Ø¹ Ø§Ù„Ø¬Ù„Ø¯ÙŠØ© Ù‚Ø¨Ù„ Ø¨Ø¶Ø¹ Ø³Ù†ÙˆØ§ØªØŒ Ù„ÙƒÙ†Ù†ÙŠ Ù„Ø³Øª Ù…ØªØ£ÙƒØ¯Ù‹Ø§ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø³Ø±Ø·Ø§Ù†Ù‹Ø§ Ø£Ù… Ù„Ø§.

Doctor: Ø³ÙŠÙƒÙˆÙ† Ù…Ù† Ø§Ù„Ù…ÙÙŠØ¯ Ù…Ø¹Ø±ÙØ© Ø°Ù„Ùƒ. ÙØ§Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¹Ø§Ø¦Ù„ÙŠ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØ²ÙŠØ¯ Ù…Ù† Ø®Ø·Ø± Ø§Ù„Ø¥ØµØ§Ø¨Ø© Ø¨Ø¨Ø¹Ø¶ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø¬Ù„Ø¯ÙŠØ©.

Patient: Ø­Ø³Ù†Ù‹Ø§ØŒ Ø³Ø£Ø­Ø§ÙˆÙ„ Ø£Ù† Ø£Ø³Ø£Ù„. Ù‡Ù„ ÙŠØ¬Ø¨ Ø£Ù† Ø£Ù‚Ù„Ù‚ØŸ

Doctor: Ù„ÙŠØ³ Ø¨Ø§Ù„Ø¶Ø±ÙˆØ±Ø©. Ù…Ø¹Ø¸Ù… Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø¬Ù„Ø¯ØŒ Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù†Øª Ø³Ø§Ø¨Ù‚Ø© Ù„Ù„ØªØ³Ø±Ø·Ù†ØŒ ÙŠÙ…ÙƒÙ† Ø¹Ù„Ø§Ø¬Ù‡Ø§ Ø¨Ø³Ù‡ÙˆÙ„Ø© Ø¥Ø°Ø§ ØªÙ… Ø§ÙƒØªØ´Ø§ÙÙ‡Ø§ Ù…Ø¨ÙƒØ±Ù‹Ø§. Ø§Ù„Ù…ÙØªØ§Ø­ Ù‡Ùˆ Ø§Ù„ÙƒØ´Ù Ø§Ù„Ù…Ø¨ÙƒØ± ÙˆØ§Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©.

Patient: Ù‡Ø°Ø§ Ù…Ø·Ù…Ø¦Ù†. Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ©ØŸ

Doctor: Ø£ÙˆÙ„Ù‹Ø§ØŒ Ø³Ø£ÙØ­Øµ Ø§Ù„Ø¨Ù‚Ø¹Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¬Ù‡Ø§Ø² ÙŠÙØ³Ù…Ù‰ "Ø§Ù„Ù…Ù†Ø¸Ø§Ø± Ø§Ù„Ø¬Ù„Ø¯ÙŠ"ØŒ ÙˆØ§Ù„Ø°ÙŠ ÙŠØ³Ø§Ø¹Ø¯Ù†ÙŠ ÙÙŠ Ø±Ø¤ÙŠØ© Ø¨Ù†ÙŠØ© Ø§Ù„Ø¬Ù„Ø¯ Ø¨ÙˆØ¶ÙˆØ­. ÙˆØ¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±ØŒ Ø³Ù†Ø£Ø®Ø° Ø®Ø²Ø¹Ø© ØµØºÙŠØ±Ø© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®Ù„Ø§ÙŠØ§ ØªØ­Øª Ø§Ù„Ù…Ø¬Ù‡Ø±.

Patient: ÙŠØ¨Ø¯Ùˆ Ø§Ù„Ø£Ù…Ø± Ø¬Ø§Ø¯Ù‹Ø§. Ù‡Ù„ Ø§Ù„Ø®Ø²Ø¹Ø© Ù…Ø¤Ù„Ù…Ø©ØŸ

Doctor: Ø¥Ù†Ù‡Ø§ Ø¹Ù…Ù„ÙŠØ© Ø¨Ø³ÙŠØ·Ø©. Ø³Ø£Ù‚ÙˆÙ… Ø¨ØªØ®Ø¯ÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø¨Ù…Ø®Ø¯Ø± Ù…ÙˆØ¶Ø¹ÙŠØŒ Ù„Ø°Ø§ Ù„Ù† ØªØ´Ø¹Ø± Ø¨Ø£ÙŠ Ø£Ù„Ù…. Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ù‡Ù†Ø§Ùƒ Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ù„Ù… Ø§Ù„Ø®ÙÙŠÙ Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ù„ÙƒÙ†Ù‡ Ø¹Ø§Ø¯Ø©Ù‹ Ù…Ø§ ÙŠÙƒÙˆÙ† Ø·ÙÙŠÙÙ‹Ø§ Ø¬Ø¯Ù‹Ø§.

Patient: Ø­Ø³Ù†Ù‹Ø§ØŒ Ù‡Ø°Ø§ Ø¬ÙŠØ¯ Ø£Ù† Ø£Ø¹Ø±Ù Ø°Ù„Ùƒ. Ø¥Ø°Ø§ ØªØ¨ÙŠÙ† Ø£Ù† Ø§Ù„Ø£Ù…Ø± Ø®Ø·ÙŠØ±ØŒ ÙÙ…Ø§ Ù‡ÙŠ Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø¹Ù„Ø§Ø¬ØŸ

Doctor: ÙŠØ¹ØªÙ…Ø¯ Ø§Ù„Ø£Ù…Ø± Ø¹Ù„Ù‰ Ø§Ù„ØªØ´Ø®ÙŠØµ Ø§Ù„Ø¯Ù‚ÙŠÙ‚. Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø­Ø§Ù„Ø© Ù…Ø¬Ø±Ø¯ "Ø§Ù„ØªÙ‚Ø±Ø§Ù† Ø§Ù„Ø³ÙØ¹ÙŠ" (ÙˆÙ‡ÙŠ Ø¨Ù‚Ø¹Ø© Ù…Ø§ Ù‚Ø¨Ù„ Ø³Ø±Ø·Ø§Ù†ÙŠØ©)ØŒ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§ Ø¨Ø§Ù„ØªØ¬Ù…ÙŠØ¯ (Ø§Ù„Ø¹Ù„Ø§Ø¬ Ø¨Ø§Ù„ØªØ¨Ø±ÙŠØ¯)ØŒ Ø£Ùˆ Ø§Ù„ÙƒØ±ÙŠÙ…Ø§Øª Ø§Ù„Ù…ÙˆØ¶Ø¹ÙŠØ©ØŒ Ø£Ùˆ Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø¨Ø³ÙŠØ·Ø© Ø£Ø®Ø±Ù‰. Ø£Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù†ÙˆØ¹Ù‹Ø§ Ù…Ù† Ø³Ø±Ø·Ø§Ù† Ø§Ù„Ø¬Ù„Ø¯ØŒ ÙÙ‚Ø¯ ÙŠØªØ·Ù„Ø¨ Ø§Ù„Ø£Ù…Ø± Ø¥Ø²Ø§Ù„Ø© Ø¬Ø±Ø§Ø­ÙŠØ©ØŒ Ø£Ùˆ Ø¹Ù„Ø§Ø¬Ù‹Ø§ Ø¨Ø§Ù„Ù„ÙŠØ²Ø±ØŒ Ø£Ùˆ Ø®ÙŠØ§Ø±Ø§Øª Ø£Ø®Ø±Ù‰.

Patient: ÙÙ‡Ù…Øª. Ù…Ø§Ø°Ø§ ÙŠØ­Ø¯Ø« Ø¥Ø°Ø§ ØªØ¬Ø§Ù‡Ù„ØªÙ‡Ø§ØŸ

Doctor: Ø¥Ø°Ø§ ÙƒØ§Ù†Øª "Ø§Ù„ØªÙ‚Ø±Ø§Ù† Ø§Ù„Ø³ÙØ¹ÙŠ"ØŒ ÙÙ‚Ø¯ ØªØªØ·ÙˆØ± Ø¥Ù„Ù‰ Ø³Ø±Ø·Ø§Ù†Ø© Ø­Ø±Ø´ÙÙŠØ© Ø§Ù„Ø®Ù„Ø§ÙŠØ§ØŒ ÙˆÙ‡ÙŠ Ø­Ø§Ù„Ø© Ø£ÙƒØ«Ø± Ø®Ø·ÙˆØ±Ø©. Ø¨Ø¹Ø¶ Ø³Ø±Ø·Ø§Ù†Ø§Øª Ø§Ù„Ø¬Ù„Ø¯ Ø§Ù„Ø£Ø®Ø±Ù‰ØŒ Ù…Ø«Ù„ Ø³Ø±Ø·Ø§Ù†Ø© Ø§Ù„Ø®Ù„Ø§ÙŠØ§ Ø§Ù„Ù‚Ø§Ø¹Ø¯ÙŠØ©ØŒ ØªÙ†Ù…Ùˆ Ø¨Ø¨Ø·Ø¡ ÙˆÙ„ÙƒÙ† ÙŠÙ…ÙƒÙ† Ø£Ù† ØªØ³Ø¨Ø¨ Ø£Ø¶Ø±Ø§Ø±Ù‹Ø§ ÙƒØ¨ÙŠØ±Ø© Ø¥Ø°Ø§ ØªÙØ±ÙƒØª Ø¯ÙˆÙ† Ø¹Ù„Ø§Ø¬.

Patient: Ø¥Ø°Ù†ØŒ ÙƒÙ„Ù…Ø§ Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§ Ù…Ø¨ÙƒØ±Ù‹Ø§ØŒ ÙƒØ§Ù† Ø°Ù„Ùƒ Ø£ÙØ¶Ù„ØŸ

Doctor: Ø¨Ø§Ù„Ø¶Ø¨Ø·. ÙƒÙ„Ù…Ø§ Ø§ÙƒØªØ´ÙÙ†Ø§ ÙˆØ¹Ø§Ù„Ø¬Ù†Ø§ Ø§Ù„Ø¢ÙØ© Ø§Ù„Ø¬Ù„Ø¯ÙŠØ© Ù…Ø¨ÙƒØ±Ù‹Ø§ØŒ ÙƒÙ„Ù…Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø¢Ù„ Ø£ÙØ¶Ù„. Ø¯Ø¹Ù†Ø§ Ù†Ù„Ù‚ÙŠ Ù†Ø¸Ø±Ø© Ø£Ù‚Ø±Ø¨ Ø§Ù„Ø¢Ù† Ø­ØªÙ‰ Ù†ØªÙ…ÙƒÙ† Ù…Ù† ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ©.
""".strip()

conversation_list = []

conversation_list.append(lognest_cnversation)

len(conversation_list)

"""# Prepare Quantization

# Loading Cohere Model
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import prepare_model_for_kbit_training
import bitsandbytes as bnb

def load_quantized_model(model_name: str, load_in_4bit: bool = True,
                         use_double_quant: bool = True,
                         quant_type: str = "nf4",
                         compute_dtype=torch.bfloat16,
                         auth_token: bool = True):

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=load_in_4bit,
        bnb_4bit_use_double_quant=use_double_quant,
        bnb_4bit_quant_type=quant_type,
        bnb_4bit_compute_dtype=compute_dtype,
    )

    n_gpus = torch.cuda.device_count()
    max_memory = {i: '40960MB' for i in range(n_gpus)}

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto",
        max_memory=max_memory,
    )

    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=auth_token)
    tokenizer.pad_token = tokenizer.eos_token

    return model, tokenizer

# Run
model_name = base_model_id
model, tokenizer = load_quantized_model(model_name)

print(model.device)

"""# Get Max Length Token For The Model"""

def get_max_length(model):


    # Pull model configuration
    conf = model.config
    # Initialize a "max_length" variable to store maximum sequence length as null
    max_length = None
    # Find maximum sequence length in the model configuration and save it in "max_length" if found
    for length_setting in ["n_positions", "max_position_embeddings", "seq_length"]:
        max_length = getattr(model.config, length_setting, None)
        if max_length:
            print(f"Found max lenth: {max_length}")
            break
    # Set "max_length" to 1024 (default value) if maximum sequence length is not found in the model configuration
    if not max_length:
        max_length = 1024
        print(f"Using default max length: {max_length}")
    return max_length
length = get_max_length(model)

"""# Clear Cache GPU"""

torch.cuda.empty_cache()
print("GPU cache cleared.")

"""# Get Number of Token for the conversation"""

tokens = tokenizer(conversation_list[0], return_tensors="pt")  # Tokenize the text
num_tokens = tokens.input_ids.shape[1]  # Get the number of tokens
print(f"Number of tokens: {num_tokens}")

"""# Translated Task"""

ar_text = conversation_list[0]
translation_extraction_message = [
    {
        "role" : "system" ,
        "content" : "\n".join([
            "You are a professional translator.",
            "You will be provided by an Arabic text." ,
            "You have to translate the text into the English Language" ,
            "Follow the provided Scheme to generate a JSON",
            "Do not generate any introduction or conclusion."
        ])
    } ,
    {
        "role" : "user" ,
        "content" : "\n".join([
            ar_text.strip() ,
        ])
    }
]

import torch

def extract_translation(details_extraction_message, tokenizer, model, max_new_tokens=1500):


    # Tokenize the input message and move to the same device
    input_ids = tokenizer.apply_chat_template(
        details_extraction_message,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt",
    ).to(model.device)

    # Generate response tokens
    gen_tokens = model.generate(
        input_ids,
        max_new_tokens=max_new_tokens,
        do_sample=False,
        temperature=None,
    )

    # Extract the generated tokens after the input prompt
    gen_tokens = [
        output_ids[len(input_ids):] for output_ids in gen_tokens
    ]

    # Decode and clean the generated text
    gen_text = tokenizer.decode(gen_tokens[0].cpu())  # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ CPU Ù‚Ø¨Ù„ ÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ±
    gen_text = gen_text.replace("<|END_RESPONSE|>", "").replace("<|END_OF_TURN_TOKEN|>", "").strip()

    return gen_text

# Example Usage:
translated_text = extract_translation(translation_extraction_message, tokenizer, model)
print(translated_text)

"""# Summarization Task"""

class SummaryDetails(BaseModel):

    Patient_symptoms: List[str] = Field(... , min_length = 5 , max_length = 300 ,description = "Key symptoms mentioned" )
    Symptom_location: str = Field(... , min_length = 5 , max_length = 300 ,description = "Affected area" )
    Duration: str = Field(... , min_length = 5 , max_length = 300 ,description = "How long the symptoms have persisted" )
    Symptom_progression: str = Field(... , min_length = 5 , max_length = 300 ,description = "How they have changed over time" )
    Risk_factors: str = Field(... , min_length = 5 , max_length = 300 ,description = "Sun exposure, family history, etc." )

en_text = translated_text
# en_text = conversation_list[2] # in English Already
details_extraction_message = [
    {
        "role" : "system" ,
        "content" : "\n".join([
            "You are an NLP data paraser." ,
            "You will be provided by an Arabic text associated with a Pydantic scheme.",
            "Generate the ouptut as same as  language.",
            "You have to extract JSON details from text according the Pydantic details.",
            "Extract details as mentioned in text.",
            "Do not generate any introduction or conclusion.",
        ])
    } ,
    {
        "role" : "user" ,
        "content" : "\n".join([
            "## conversation" ,
            en_text.strip() ,
            "" ,
            "## Pydantic Details" ,
            json.dumps(
                SummaryDetails.model_json_schema() , ensure_ascii = False
            ) ,
            "" ,
            "## Summarization : " ,
            "```json"
        ])
    }
]

def extract_details(details_extraction_message, tokenizer, model, max_new_tokens=200):
    # Tokenize the input message
    input_ids = tokenizer.apply_chat_template(
        details_extraction_message,
        tokenize=True,
        add_generation_prompt=True,

        return_tensors="pt",
    ).to(model.device)

    # Generate response tokens
    gen_tokens = model.generate(
        input_ids,
        max_new_tokens=max_new_tokens,
        do_sample=False,
        temperature=None,
    )

    # Extract the generated tokens after the input prompt
    gen_tokens = [
        output_ids[len(input_ids):]
        for input_ids, output_ids in zip(input_ids, gen_tokens)
    ]

    # Decode and clean the generated text
    gen_text = tokenizer.decode(gen_tokens[0])
    gen_text = gen_text.replace("<|END_RESPONSE|>", "").replace("<|END_OF_TURN_TOKEN|>", "").replace("```" , "").strip()

    return gen_text

# Example Usage:
generated_text = extract_details(details_extraction_message, tokenizer, model)
print(generated_text)

"""#Create ApI"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import nest_asyncio
from pyngrok import ngrok
import uvicorn

# Apply asyncio patch for Colab
nest_asyncio.apply()

app = FastAPI()


# Define input format
class SummarizationInput(BaseModel):
    arabic_text: str

@app.post("/summarize")
def summarize(input_data: SummarizationInput):
    try:
        # Step 1: Translate
        translation_extraction_message = [
            {"role": "system", "content": "Translate Arabic to English..."},
            {"role": "user", "content": input_data.arabic_text.strip()}
        ]
        en_text = extract_translation(translation_extraction_message, tokenizer, model)

        # Step 2: Extract summary
        details_extraction_message = [
            {"role": "system", "content": "Extract details using Pydantic schema..."},
            {"role": "user", "content": "\n".join([
                "## conversation",
                en_text,
                "## Pydantic Details",
                json.dumps(SummaryDetails.model_json_schema(), ensure_ascii=False),
                "## Summarization : ",
                "```json"
            ])}
        ]
        summary = extract_details(details_extraction_message, tokenizer, model)

        return json.loads(summary)

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

nest_asyncio.apply()
NGROK_AUTH_TOKEN = userdata.get("NGROK_AUTH_TOKEN")
ngrok.set_auth_token(NGROK_AUTH_TOKEN)
for tunnel in ngrok.get_tunnels():
    ngrok.disconnect(tunnel.public_url)
public_url = ngrok.connect(8001)
print("ğŸš€ Your API is live at:", public_url)
uvicorn.run(app, host="0.0.0.0", port=8001)

!lsof -i:8001

!kill -9 <PID>

"""#Save Summarization to json file and upload it on Gdrive"""

# # Define the filename
# file_path = "/content/gdrive/MyDrive/summarization.json"

# # Load existing data if the file exists
# if os.path.exists(file_path):
#     with open(file_path, "r", encoding="utf-8") as json_file:
#         try:
#             existing_data = json.load(json_file)
#             if not isinstance(existing_data, list):
#                 existing_data = [existing_data]  # Ensure it's a list
#         except json.JSONDecodeError:
#             existing_data = []
# else:
#     existing_data = []

# # Convert the new string data to a Python object
# new_data = json.loads(generated_text)

# # Append the new data
# existing_data.append(new_data)

# # Save back to JSON
# with open(file_path, "w", encoding="utf-8") as json_file:
#     json.dump(existing_data, json_file, indent=4, ensure_ascii=False)

# print(f"Output appended to {file_path}")

"""# Add with ID to json file"""

# # Define the filename
# file_path = "/content/gdrive/MyDrive/summarization.json"
# # Load existing data if the file exists
# if os.path.exists(file_path):
#     with open(file_path, "r", encoding="utf-8") as json_file:
#         try:
#             existing_data = json.load(json_file)
#             if not isinstance(existing_data, list):
#                 existing_data = [existing_data]  # Ensure it's a list
#         except json.JSONDecodeError:
#             existing_data = []
# else:
#     existing_data = []

# # Convert the new string data to a Python object
# new_data = json.loads(generated_text)

# # Assign an ID based on the existing data length
# new_id = existing_data[-1]["id"] + 1 if existing_data else 1

# # Create an ordered dictionary to ensure "id" comes first
# ordered_data = OrderedDict([("id", new_id)])
# ordered_data.update(new_data)  # Add the rest of the keys

# # Append the new data
# existing_data.append(ordered_data)

# # Save back to JSON
# with open(file_path, "w", encoding="utf-8") as json_file:
#     json.dump(existing_data, json_file, indent=4, ensure_ascii=False)

# print(f"Output appended to {file_path} with ID {new_id}")