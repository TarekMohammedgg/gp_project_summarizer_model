# -*- coding: utf-8 -*-
"""draft+api

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1geebdx60V4zEa6FNfxlkc_glWBy0Jspe

# Resources

resouces : https://docs.cohere.com/v2/changelog/command-r7b-arabic

translator Model resource : https://huggingface.co/Helsinki-NLP/opus-mt-ar-en/tree/main

# convert notebook to ipynb
"""

!jupyter nbconvert --to script your_notebook_name.ipynb

"""# pips"""

!pip install -q fastapi nest-asyncio pyngrok uvicorn
!pip install -q bitsandbytes
!pip install -q accelerate==0.26.0 --progress-bar off
!pip install fastapi nest-asyncio pyngrok uvicorn

# !pip install -U -q bitsandbytes transformers accelerate

"""# Import Liberaries"""

from pydantic import BaseModel, Field
from typing import List, Optional, Literal
import json
from collections import OrderedDict
from transformers import AutoTokenizer, AutoModelForCausalLM , BitsAndBytesConfig
import torch
import torch._dynamo
import os
from google.colab import drive
from huggingface_hub import login
import nest_asyncio
from pyngrok import ngrok
import uvicorn
from google.colab import userdata
nest_asyncio.apply()
hf_token  = "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"
base_model_id = "CohereForAI/c4ai-command-r7b-arabic-02-2025"
torch_dtype = torch.float16
login(token=hf_token)
torch._dynamo.config.suppress_errors = True
torch._dynamo.config.disable = True

"""# Read Json file"""

# drive.mount('/content/gdrive')

# # Define the file path
# file_path = "/content/gdrive/MyDrive/medical_conv.json"

# # Check if file exists before reading
# if os.path.exists(file_path):
#     with open(file_path, "r", encoding="utf-8") as json_file:
#         try:
#             data = json.load(json_file)

#             # Create a list to store formatted conversations
#             conversation_list = [f"{item['conversation']}".replace("\n" , " ") for i, item in enumerate(data, start=1)]

#             # Print the list
#             print(conversation_list)

#         except json.JSONDecodeError as e:
#             print(f"Error decoding JSON: {e}")
# else:
#     print("File not found!")

lognest_cnversation = """
Doctor: صباح الخير! ما الذي أتى بك اليوم؟

Patient: صباح الخير، دكتور. لاحظت وجود بقعة خشنة ومتقشرة على جبهتي منذ بضعة أشهر. بدأت صغيرة، لكنني أعتقد أنها تكبر.

Doctor: فهمت. هل يمكنك وصف كيف تشعر بها؟ هل هي مثيرة للحكة، مؤلمة، أم حساسة عند لمسها؟

Patient: إنها خشنة وجافة في الغالب. أحيانًا أشعر بحكة خفيفة، وإذا حككتها كثيرًا، تنزف قليلًا.

Doctor: منذ متى وأنت تلاحظ هذا التغير؟ هل لاحظت أي تغييرات في الحجم أو اللون أو الملمس بمرور الوقت؟

Patient: لاحظتها لأول مرة منذ حوالي ستة أشهر. في البداية، كانت بالكاد تُرى، لكنها الآن أصبحت أكثر خشونة ويبدو أنها أكبر قليلاً.

Doctor: هل سبق لك أن عانيت من بقع مشابهة على بشرتك من قبل؟

Patient: لا، هذه هي المرة الأولى التي يحدث لي فيها شيء كهذا.

Doctor: هل تقضي وقتًا طويلاً في الهواء الطلق؟ هل تعرضت لأشعة الشمس بشكل كبير على مر السنين؟

Patient: نعم، أعمل في البناء، لذا أتعرض للشمس يوميًا لساعات طويلة.

Doctor: هل تستخدم واقي الشمس بانتظام أو ترتدي ملابس واقية مثل القبعات؟

Patient: بصراحة، لا. أرتدي قبعة أحيانًا، لكن لا أستخدم واقي الشمس. لم أكن أعتقد أنه مهم.

Doctor: التعرض لأشعة الشمس عامل رئيسي في تلف الجلد، واستخدام واقي الشمس يمكن أن يقلل بشكل كبير من خطر الإصابة بمشاكل الجلد، بما في ذلك الآفات السابقة للتسرطن وسرطان الجلد.

Patient: لم أكن أعلم أنه أمر خطير لهذه الدرجة. هل تعتقد أن هذه البقعة قد تكون خطيرة؟

Doctor: لا يمكنني الجزم بذلك دون فحص دقيق، ولكن من الجيد أنك جئت مبكرًا. بعض آفات الجلد قد تكون غير ضارة، ولكن البعض الآخر قد يتطلب علاجًا لمنع حدوث مضاعفات.

Patient: هذا مقلق بعض الشيء. هل هذا يعني أنه قد يكون سرطانًا؟

Doctor: ليس بالضرورة، ولكن بعض أنواع سرطان الجلد تبدأ كبقع خشنة ومتقشرة مثل ما تصفه. سأحتاج إلى فحصها عن كثب وربما أخذ خزعة لتأكيد التشخيص.

Patient: حسنًا، هذا منطقي. هل هناك أعراض أخرى يجب أن أراقبها؟

Doctor: نعم، إذا بدأت البقعة في التغير بسرعة في الحجم أو اللون أو الشكل، أو إذا أصبحت مؤلمة أو بدأت في الإفراز أو تطورت إلى قرحة، فقد تكون هذه علامات تحذيرية.

Patient: فهمت. حسنًا، لم ألاحظ أي إفرازات أو تغيرات كبيرة في اللون.

Doctor: هذا جيد. هل لاحظت ظهور أي شامات أو بقع جديدة على جسمك مؤخرًا؟

Patient: لا، ليس على حد علمي.

Doctor: هل لديك تاريخ سابق مع مشاكل الجلد مثل الطفح الجلدي أو الأكزيما؟

Patient: لا، بشرتي كانت دائمًا طبيعية.

Doctor: هل يوجد في عائلتك أي شخص لديه تاريخ من سرطان الجلد أو أي مشاكل جلدية أخرى؟

Patient: والدي أزال بعض البقع الجلدية قبل بضع سنوات، لكنني لست متأكدًا مما إذا كانت سرطانًا أم لا.

Doctor: سيكون من المفيد معرفة ذلك. فالتاريخ العائلي يمكن أن يزيد من خطر الإصابة ببعض الحالات الجلدية.

Patient: حسنًا، سأحاول أن أسأل. هل يجب أن أقلق؟

Doctor: ليس بالضرورة. معظم مشاكل الجلد، حتى لو كانت سابقة للتسرطن، يمكن علاجها بسهولة إذا تم اكتشافها مبكرًا. المفتاح هو الكشف المبكر والإدارة الصحيحة.

Patient: هذا مطمئن. ما هي الخطوة التالية؟

Doctor: أولًا، سأفحص البقعة باستخدام جهاز يُسمى "المنظار الجلدي"، والذي يساعدني في رؤية بنية الجلد بوضوح. وإذا لزم الأمر، سنأخذ خزعة صغيرة لتحليل الخلايا تحت المجهر.

Patient: يبدو الأمر جادًا. هل الخزعة مؤلمة؟

Doctor: إنها عملية بسيطة. سأقوم بتخدير المنطقة بمخدر موضعي، لذا لن تشعر بأي ألم. قد يكون هناك بعض الألم الخفيف بعد ذلك، لكنه عادةً ما يكون طفيفًا جدًا.

Patient: حسنًا، هذا جيد أن أعرف ذلك. إذا تبين أن الأمر خطير، فما هي خيارات العلاج؟

Doctor: يعتمد الأمر على التشخيص الدقيق. إذا كانت الحالة مجرد "التقران السفعي" (وهي بقعة ما قبل سرطانية)، يمكننا معالجتها بالتجميد (العلاج بالتبريد)، أو الكريمات الموضعية، أو إجراءات بسيطة أخرى. أما إذا كانت نوعًا من سرطان الجلد، فقد يتطلب الأمر إزالة جراحية، أو علاجًا بالليزر، أو خيارات أخرى.

Patient: فهمت. ماذا يحدث إذا تجاهلتها؟

Doctor: إذا كانت "التقران السفعي"، فقد تتطور إلى سرطانة حرشفية الخلايا، وهي حالة أكثر خطورة. بعض سرطانات الجلد الأخرى، مثل سرطانة الخلايا القاعدية، تنمو ببطء ولكن يمكن أن تسبب أضرارًا كبيرة إذا تُركت دون علاج.

Patient: إذن، كلما عالجتها مبكرًا، كان ذلك أفضل؟

Doctor: بالضبط. كلما اكتشفنا وعالجنا الآفة الجلدية مبكرًا، كلما كان المآل أفضل. دعنا نلقي نظرة أقرب الآن حتى نتمكن من تحديد الخطوة التالية.
""".strip()

conversation_list = []

conversation_list.append(lognest_cnversation)

len(conversation_list)

"""# Prepare Quantization

# Loading Cohere Model
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import prepare_model_for_kbit_training
import bitsandbytes as bnb

def load_quantized_model(model_name: str, load_in_4bit: bool = True,
                         use_double_quant: bool = True,
                         quant_type: str = "nf4",
                         compute_dtype=torch.bfloat16,
                         auth_token: bool = True):

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=load_in_4bit,
        bnb_4bit_use_double_quant=use_double_quant,
        bnb_4bit_quant_type=quant_type,
        bnb_4bit_compute_dtype=compute_dtype,
    )

    n_gpus = torch.cuda.device_count()
    max_memory = {i: '40960MB' for i in range(n_gpus)}

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto",
        max_memory=max_memory,
    )

    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=auth_token)
    tokenizer.pad_token = tokenizer.eos_token

    return model, tokenizer

# Run
model_name = base_model_id
model, tokenizer = load_quantized_model(model_name)

print(model.device)

"""# Get Max Length Token For The Model"""

def get_max_length(model):


    # Pull model configuration
    conf = model.config
    # Initialize a "max_length" variable to store maximum sequence length as null
    max_length = None
    # Find maximum sequence length in the model configuration and save it in "max_length" if found
    for length_setting in ["n_positions", "max_position_embeddings", "seq_length"]:
        max_length = getattr(model.config, length_setting, None)
        if max_length:
            print(f"Found max lenth: {max_length}")
            break
    # Set "max_length" to 1024 (default value) if maximum sequence length is not found in the model configuration
    if not max_length:
        max_length = 1024
        print(f"Using default max length: {max_length}")
    return max_length
length = get_max_length(model)

"""# Clear Cache GPU"""

torch.cuda.empty_cache()
print("GPU cache cleared.")

"""# Get Number of Token for the conversation"""

tokens = tokenizer(conversation_list[0], return_tensors="pt")  # Tokenize the text
num_tokens = tokens.input_ids.shape[1]  # Get the number of tokens
print(f"Number of tokens: {num_tokens}")

"""# Translated Task"""

ar_text = conversation_list[0]
translation_extraction_message = [
    {
        "role" : "system" ,
        "content" : "\n".join([
            "You are a professional translator.",
            "You will be provided by an Arabic text." ,
            "You have to translate the text into the English Language" ,
            "Follow the provided Scheme to generate a JSON",
            "Do not generate any introduction or conclusion."
        ])
    } ,
    {
        "role" : "user" ,
        "content" : "\n".join([
            ar_text.strip() ,
        ])
    }
]

import torch

def extract_translation(details_extraction_message, tokenizer, model, max_new_tokens=1500):


    # Tokenize the input message and move to the same device
    input_ids = tokenizer.apply_chat_template(
        details_extraction_message,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt",
    ).to(model.device)

    # Generate response tokens
    gen_tokens = model.generate(
        input_ids,
        max_new_tokens=max_new_tokens,
        do_sample=False,
        temperature=None,
    )

    # Extract the generated tokens after the input prompt
    gen_tokens = [
        output_ids[len(input_ids):] for output_ids in gen_tokens
    ]

    # Decode and clean the generated text
    gen_text = tokenizer.decode(gen_tokens[0].cpu())  # تحويل النتائج إلى CPU قبل فك التشفير
    gen_text = gen_text.replace("<|END_RESPONSE|>", "").replace("<|END_OF_TURN_TOKEN|>", "").strip()

    return gen_text

# Example Usage:
translated_text = extract_translation(translation_extraction_message, tokenizer, model)
print(translated_text)

"""# Summarization Task"""

class SummaryDetails(BaseModel):

    Patient_symptoms: List[str] = Field(... , min_length = 5 , max_length = 300 ,description = "Key symptoms mentioned" )
    Symptom_location: str = Field(... , min_length = 5 , max_length = 300 ,description = "Affected area" )
    Duration: str = Field(... , min_length = 5 , max_length = 300 ,description = "How long the symptoms have persisted" )
    Symptom_progression: str = Field(... , min_length = 5 , max_length = 300 ,description = "How they have changed over time" )
    Risk_factors: str = Field(... , min_length = 5 , max_length = 300 ,description = "Sun exposure, family history, etc." )

en_text = translated_text
# en_text = conversation_list[2] # in English Already
details_extraction_message = [
    {
        "role" : "system" ,
        "content" : "\n".join([
            "You are an NLP data paraser." ,
            "You will be provided by an Arabic text associated with a Pydantic scheme.",
            "Generate the ouptut as same as  language.",
            "You have to extract JSON details from text according the Pydantic details.",
            "Extract details as mentioned in text.",
            "Do not generate any introduction or conclusion.",
        ])
    } ,
    {
        "role" : "user" ,
        "content" : "\n".join([
            "## conversation" ,
            en_text.strip() ,
            "" ,
            "## Pydantic Details" ,
            json.dumps(
                SummaryDetails.model_json_schema() , ensure_ascii = False
            ) ,
            "" ,
            "## Summarization : " ,
            "```json"
        ])
    }
]

def extract_details(details_extraction_message, tokenizer, model, max_new_tokens=200):
    # Tokenize the input message
    input_ids = tokenizer.apply_chat_template(
        details_extraction_message,
        tokenize=True,
        add_generation_prompt=True,

        return_tensors="pt",
    ).to(model.device)

    # Generate response tokens
    gen_tokens = model.generate(
        input_ids,
        max_new_tokens=max_new_tokens,
        do_sample=False,
        temperature=None,
    )

    # Extract the generated tokens after the input prompt
    gen_tokens = [
        output_ids[len(input_ids):]
        for input_ids, output_ids in zip(input_ids, gen_tokens)
    ]

    # Decode and clean the generated text
    gen_text = tokenizer.decode(gen_tokens[0])
    gen_text = gen_text.replace("<|END_RESPONSE|>", "").replace("<|END_OF_TURN_TOKEN|>", "").replace("```" , "").strip()

    return gen_text

# Example Usage:
generated_text = extract_details(details_extraction_message, tokenizer, model)
print(generated_text)

"""#Create ApI"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import nest_asyncio
from pyngrok import ngrok
import uvicorn

# Apply asyncio patch for Colab
nest_asyncio.apply()

app = FastAPI()


# Define input format
class SummarizationInput(BaseModel):
    arabic_text: str

@app.post("/summarize")
def summarize(input_data: SummarizationInput):
    try:
        # Step 1: Translate
        translation_extraction_message = [
            {"role": "system", "content": "Translate Arabic to English..."},
            {"role": "user", "content": input_data.arabic_text.strip()}
        ]
        en_text = extract_translation(translation_extraction_message, tokenizer, model)

        # Step 2: Extract summary
        details_extraction_message = [
            {"role": "system", "content": "Extract details using Pydantic schema..."},
            {"role": "user", "content": "\n".join([
                "## conversation",
                en_text,
                "## Pydantic Details",
                json.dumps(SummaryDetails.model_json_schema(), ensure_ascii=False),
                "## Summarization : ",
                "```json"
            ])}
        ]
        summary = extract_details(details_extraction_message, tokenizer, model)

        return json.loads(summary)

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

nest_asyncio.apply()
NGROK_AUTH_TOKEN = userdata.get("NGROK_AUTH_TOKEN")
ngrok.set_auth_token(NGROK_AUTH_TOKEN)
for tunnel in ngrok.get_tunnels():
    ngrok.disconnect(tunnel.public_url)
public_url = ngrok.connect(8001)
print("🚀 Your API is live at:", public_url)
uvicorn.run(app, host="0.0.0.0", port=8001)

!lsof -i:8001

!kill -9 <PID>

"""#Save Summarization to json file and upload it on Gdrive"""

# # Define the filename
# file_path = "/content/gdrive/MyDrive/summarization.json"

# # Load existing data if the file exists
# if os.path.exists(file_path):
#     with open(file_path, "r", encoding="utf-8") as json_file:
#         try:
#             existing_data = json.load(json_file)
#             if not isinstance(existing_data, list):
#                 existing_data = [existing_data]  # Ensure it's a list
#         except json.JSONDecodeError:
#             existing_data = []
# else:
#     existing_data = []

# # Convert the new string data to a Python object
# new_data = json.loads(generated_text)

# # Append the new data
# existing_data.append(new_data)

# # Save back to JSON
# with open(file_path, "w", encoding="utf-8") as json_file:
#     json.dump(existing_data, json_file, indent=4, ensure_ascii=False)

# print(f"Output appended to {file_path}")

"""# Add with ID to json file"""

# # Define the filename
# file_path = "/content/gdrive/MyDrive/summarization.json"
# # Load existing data if the file exists
# if os.path.exists(file_path):
#     with open(file_path, "r", encoding="utf-8") as json_file:
#         try:
#             existing_data = json.load(json_file)
#             if not isinstance(existing_data, list):
#                 existing_data = [existing_data]  # Ensure it's a list
#         except json.JSONDecodeError:
#             existing_data = []
# else:
#     existing_data = []

# # Convert the new string data to a Python object
# new_data = json.loads(generated_text)

# # Assign an ID based on the existing data length
# new_id = existing_data[-1]["id"] + 1 if existing_data else 1

# # Create an ordered dictionary to ensure "id" comes first
# ordered_data = OrderedDict([("id", new_id)])
# ordered_data.update(new_data)  # Add the rest of the keys

# # Append the new data
# existing_data.append(ordered_data)

# # Save back to JSON
# with open(file_path, "w", encoding="utf-8") as json_file:
#     json.dump(existing_data, json_file, indent=4, ensure_ascii=False)

# print(f"Output appended to {file_path} with ID {new_id}")